{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb9503",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/jonat/Desktop/employee_churn_prediction/employee_churn_prediction/venv/Scripts/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Predicción de Abandono de Clientes Bancarios\n",
    "# Autor: Jonathan Ibáñez\n",
    "# Fecha: Septiembre 2025\n",
    "# \n",
    "# Modelo de Machine Learning para predecir abandono de clientes bancarios\n",
    "# Modelo final alcanza AUC: 0.88\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# =============================================================================\n",
    "# CARGA Y EXPLORACIÓN DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "# Cargar dataset\n",
    "df = pd.read_csv('../data/bank_customer_churn.csv')\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Uso de memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Información básica\n",
    "print(\"\\nResumen del dataset:\")\n",
    "print(f\"Total de clientes: {df.shape[0]:,}\")\n",
    "print(f\"Características disponibles: {df.shape[1]}\")\n",
    "print(f\"Variables numéricas: {df.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "print(f\"Variables categóricas: {df.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "# Análisis de variable objetivo\n",
    "distribucion_abandono = df['Exited'].value_counts()\n",
    "tasa_abandono = df['Exited'].value_counts(normalize=True)\n",
    "\n",
    "print(f\"\\nAnálisis de abandono:\")\n",
    "print(f\"Clientes retenidos: {distribucion_abandono[0]:,} ({tasa_abandono[0]:.1%})\")\n",
    "print(f\"Clientes que abandonaron: {distribucion_abandono[1]:,} ({tasa_abandono[1]:.1%})\")\n",
    "\n",
    "# Visualizar distribución objetivo\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "distribucion_abandono.plot(kind='bar', ax=ax1, color=['green', 'red'], alpha=0.7)\n",
    "ax1.set_title('Distribución de Abandono de Clientes')\n",
    "ax1.set_xlabel('Estado del Cliente')\n",
    "ax1.set_ylabel('Cantidad')\n",
    "ax1.set_xticklabels(['Retenidos', 'Abandonaron'], rotation=0)\n",
    "\n",
    "ax2.pie(distribucion_abandono.values, labels=['Retenidos', 'Abandonaron'], \n",
    "        autopct='%1.1f%%', colors=['green', 'red'])\n",
    "ax2.set_title('Distribución de Tasa de Abandono')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESAMIENTO DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "def preparar_caracteristicas(df):\n",
    "    \"\"\"\n",
    "    Preparar características para modelos de machine learning\n",
    "    \"\"\"\n",
    "    df_procesado = df.copy()\n",
    "    \n",
    "    # Identificar columnas categóricas y numéricas\n",
    "    columnas_categoricas = df_procesado.select_dtypes(include=['object']).columns.tolist()\n",
    "    columnas_numericas = df_procesado.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remover columnas innecesarias\n",
    "    columnas_excluir = ['RowNumber', 'CustomerId', 'Surname', 'Exited']\n",
    "    columnas_categoricas = [col for col in columnas_categoricas if col not in columnas_excluir]\n",
    "    columnas_numericas = [col for col in columnas_numericas if col not in columnas_excluir]\n",
    "    \n",
    "    print(f\"Características categóricas: {len(columnas_categoricas)}\")\n",
    "    print(f\"Características numéricas: {len(columnas_numericas)}\")\n",
    "    \n",
    "    # Codificar variables categóricas\n",
    "    codificadores = {}\n",
    "    for col in columnas_categoricas:\n",
    "        le = LabelEncoder()\n",
    "        df_procesado[col] = le.fit_transform(df_procesado[col])\n",
    "        codificadores[col] = le\n",
    "        print(f\"Codificado {col}: {len(le.classes_)} valores únicos\")\n",
    "    \n",
    "    # Selección final de características\n",
    "    columnas_caracteristicas = columnas_categoricas + columnas_numericas\n",
    "    X = df_procesado[columnas_caracteristicas]\n",
    "    y = df_procesado['Exited']\n",
    "    \n",
    "    print(f\"\\nDataset final:\")\n",
    "    print(f\"Características (X): {X.shape}\")\n",
    "    print(f\"Variable objetivo (y): {y.shape}\")\n",
    "    print(f\"Distribución de clases: {np.bincount(y)}\")\n",
    "    \n",
    "    return X, y, columnas_caracteristicas, codificadores\n",
    "\n",
    "# Procesar características\n",
    "X, y, nombres_caracteristicas, codificadores = preparar_caracteristicas(df)\n",
    "\n",
    "# División entrenamiento-prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nDivisión de datos completada:\")\n",
    "print(f\"Conjunto entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Escalado de características para algoritmos que lo requieren\n",
    "escalador = StandardScaler()\n",
    "X_train_escalado = escalador.fit_transform(X_train)\n",
    "X_test_escalado = escalador.transform(X_test)\n",
    "\n",
    "print(f\"Escalado de características completado\")\n",
    "print(f\"Características escaladas - Media: {X_train_escalado.mean():.2f}, Std: {X_train_escalado.std():.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRENAMIENTO Y COMPARACIÓN DE MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "# Inicializar modelos\n",
    "modelos = {\n",
    "    'SVM': SVC(C=1.0, kernel='rbf', probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                               random_state=42, verbose=-1),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar modelos\n",
    "resultados = {}\n",
    "objetos_modelo = {}\n",
    "\n",
    "print(\"Entrenando modelos de machine learning:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    print(f\"\\nEntrenando {nombre}...\")\n",
    "    \n",
    "    # Usar datos escalados para SVM y KNN\n",
    "    if nombre in ['SVM', 'KNN']:\n",
    "        modelo.fit(X_train_escalado, y_train)\n",
    "        y_prob = modelo.predict_proba(X_test_escalado)[:, 1]\n",
    "        y_pred = modelo.predict(X_test_escalado)\n",
    "    else:\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "        y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    puntaje_auc = roc_auc_score(y_test, y_prob)\n",
    "    precision = (y_pred == y_test).mean()\n",
    "    \n",
    "    resultados[nombre] = puntaje_auc\n",
    "    objetos_modelo[nombre] = modelo\n",
    "    \n",
    "    print(f\"Puntaje AUC: {puntaje_auc:.4f}\")\n",
    "    print(f\"Precisión: {precision:.4f}\")\n",
    "\n",
    "# Resumen de resultados\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS DE COMPARACIÓN DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "resultados_ordenados = dict(sorted(resultados.items(), key=lambda x: x[1], reverse=True))\n",
    "print(\"Ranking por Puntaje AUC:\")\n",
    "for i, (nombre_modelo, puntaje_auc) in enumerate(resultados_ordenados.items(), 1):\n",
    "    print(f\"{i}. {nombre_modelo:<15} AUC: {puntaje_auc:.4f}\")\n",
    "\n",
    "mejor_modelo_nombre = max(resultados, key=resultados.get)\n",
    "print(f\"\\nMejor modelo: {mejor_modelo_nombre} (AUC: {resultados[mejor_modelo_nombre]:.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMIZACIÓN DE HIPERPARÁMETROS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nOptimizando hiperparámetros de {mejor_modelo_nombre}...\")\n",
    "\n",
    "# Definir grilla de parámetros para LightGBM\n",
    "grilla_parametros = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [6, 9],\n",
    "    'num_leaves': [15, 31]\n",
    "}\n",
    "\n",
    "# Búsqueda en grilla\n",
    "busqueda_grilla = GridSearchCV(\n",
    "    LGBMClassifier(random_state=42, verbose=-1),\n",
    "    grilla_parametros,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "busqueda_grilla.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nOptimización completada!\")\n",
    "print(f\"Mejores parámetros: {busqueda_grilla.best_params_}\")\n",
    "print(f\"Mejor puntaje CV: {busqueda_grilla.best_score_:.4f}\")\n",
    "\n",
    "# Evaluar modelo optimizado\n",
    "modelo_optimizado = busqueda_grilla.best_estimator_\n",
    "y_prob_optimizado = modelo_optimizado.predict_proba(X_test)[:, 1]\n",
    "y_pred_optimizado = modelo_optimizado.predict(X_test)\n",
    "auc_optimizado = roc_auc_score(y_test, y_prob_optimizado)\n",
    "\n",
    "print(f\"AUC del modelo optimizado en conjunto de prueba: {auc_optimizado:.4f}\")\n",
    "print(f\"Mejora: {auc_optimizado - resultados[mejor_modelo_nombre]:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUACIÓN E INTERPRETACIÓN DEL MODELO\n",
    "# =============================================================================\n",
    "\n",
    "# Evaluación detallada de métricas\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred_optimizado)\n",
    "recall = recall_score(y_test, y_pred_optimizado)\n",
    "f1 = f1_score(y_test, y_pred_optimizado)\n",
    "precision_final = (y_pred_optimizado == y_test).mean()\n",
    "\n",
    "print(f\"\\nRendimiento del Modelo Final:\")\n",
    "print(f\"Puntaje AUC: {auc_optimizado:.4f}\")\n",
    "print(f\"Precisión: {precision_final:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_optimizado)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Retenidos', 'Abandonaron'], \n",
    "            yticklabels=['Retenidos', 'Abandonaron'])\n",
    "plt.title('Matriz de Confusión - Modelo LightGBM Optimizado')\n",
    "plt.ylabel('Real')\n",
    "plt.xlabel('Predicción')\n",
    "plt.show()\n",
    "\n",
    "# Análisis de importancia de características\n",
    "df_importancia = pd.DataFrame({\n",
    "    'caracteristica': nombres_caracteristicas,\n",
    "    'importancia': modelo_optimizado.feature_importances_\n",
    "}).sort_values('importancia', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Características Más Importantes:\")\n",
    "for i, row in df_importancia.head(10).iterrows():\n",
    "    print(f\"{i+1:2d}. {row['caracteristica']:<20} {row['importancia']:.4f}\")\n",
    "\n",
    "# Visualizar importancia de características\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_caracteristicas = df_importancia.head(10)\n",
    "plt.barh(range(len(top_caracteristicas)), top_caracteristicas['importancia'])\n",
    "plt.yticks(range(len(top_caracteristicas)), top_caracteristicas['caracteristica'])\n",
    "plt.xlabel('Importancia de Características')\n",
    "plt.title('Top 10 Características Más Importantes - Modelo LightGBM')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ANÁLISIS DE IMPACTO EMPRESARIAL\n",
    "# =============================================================================\n",
    "\n",
    "# Segmentación de riesgo\n",
    "probabilidades_riesgo = y_prob_optimizado\n",
    "clientes_alto_riesgo = np.sum(probabilidades_riesgo > 0.7)\n",
    "clientes_riesgo_medio = np.sum((probabilidades_riesgo > 0.4) & (probabilidades_riesgo <= 0.7))\n",
    "clientes_bajo_riesgo = np.sum(probabilidades_riesgo <= 0.4)\n",
    "\n",
    "print(f\"\\nResultados de Segmentación de Riesgo:\")\n",
    "print(f\"Clientes alto riesgo (>70%): {clientes_alto_riesgo} ({clientes_alto_riesgo/len(probabilidades_riesgo)*100:.1f}%)\")\n",
    "print(f\"Clientes riesgo medio (40-70%): {clientes_riesgo_medio} ({clientes_riesgo_medio/len(probabilidades_riesgo)*100:.1f}%)\")\n",
    "print(f\"Clientes bajo riesgo (<40%): {clientes_bajo_riesgo} ({clientes_bajo_riesgo/len(probabilidades_riesgo)*100:.1f}%)\")\n",
    "\n",
    "# Simulación de impacto empresarial\n",
    "total_clientes = len(df)\n",
    "tasa_abandono_anual = df['Exited'].sum() / total_clientes\n",
    "clientes_abandonan_anualmente = int(total_clientes * tasa_abandono_anual)\n",
    "clientes_detectados = int(clientes_abandonan_anualmente * recall)\n",
    "\n",
    "# Impacto financiero (valores ejemplo)\n",
    "valor_promedio_cliente = 1200  # Valor anual por cliente\n",
    "costo_retencion = 50          # Costo de campaña de retención\n",
    "costo_adquisicion = 200       # Costo de adquirir nuevo cliente\n",
    "\n",
    "ingresos_salvados = clientes_detectados * valor_promedio_cliente\n",
    "costo_campana = clientes_alto_riesgo * costo_retencion\n",
    "beneficio_neto = ingresos_salvados - costo_campana\n",
    "\n",
    "print(f\"\\nSimulación de Impacto Empresarial:\")\n",
    "print(f\"Total de clientes: {total_clientes:,}\")\n",
    "print(f\"Tasa de abandono anual: {tasa_abandono_anual:.1%}\")\n",
    "print(f\"Clientes que abandonan anualmente: {clientes_abandonan_anualmente}\")\n",
    "print(f\"Clientes detectados por el modelo: {clientes_detectados} ({recall:.1%} tasa de detección)\")\n",
    "print(f\"Ingresos salvados: {ingresos_salvados:,}€\")\n",
    "print(f\"Costos de campaña: {costo_campana:,}€\")\n",
    "print(f\"Beneficio neto: {beneficio_neto:,}€\")\n",
    "print(f\"ROI: {(beneficio_neto/costo_campana)*100:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# PERSISTENCIA DEL MODELO\n",
    "# =============================================================================\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "archivo_modelo = f\"../models/modelo_abandono_bancario_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "joblib.dump(modelo_optimizado, archivo_modelo)\n",
    "\n",
    "# Guardar el escalador\n",
    "archivo_escalador = f\"../models/escalador_caracteristicas_{datetime.now().strftime('%Y%m%d')}.pkl\"\n",
    "joblib.dump(escalador, archivo_escalador)\n",
    "\n",
    "print(\"¡Modelo entrenado listo para usarse!\")\n",
    "\n",
    "print(f\"\\nModelo guardado exitosamente:\")\n",
    "print(f\"Archivo modelo: {archivo_modelo}\")\n",
    "print(f\"Archivo escalador: {archivo_escalador}\")\n",
    "\n",
    "# Metadatos del modelo\n",
    "metadatos_modelo = {\n",
    "    'tipo_modelo': 'LightGBM',\n",
    "    'rendimiento': {\n",
    "        'puntaje_auc': float(auc_optimizado),\n",
    "        'precision': float(precision_final),\n",
    "        'precision_score': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1)\n",
    "    },\n",
    "    'hiperparametros': busqueda_grilla.best_params_,\n",
    "    'nombres_caracteristicas': nombres_caracteristicas,\n",
    "    'fecha_entrenamiento': datetime.now().isoformat(),\n",
    "    'impacto_empresarial': {\n",
    "        'tasa_deteccion': float(recall),\n",
    "        'clientes_alto_riesgo': int(clientes_alto_riesgo),\n",
    "        'beneficio_anual_estimado': int(beneficio_neto)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nProyecto completado exitosamente!\")\n",
    "print(f\"AUC final del modelo: {auc_optimizado:.4f}\")\n",
    "print(f\"Modelo listo para implementación en producción.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
